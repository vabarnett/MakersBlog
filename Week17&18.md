This week we started the final project of the bootcamp. I was really pleased with the group I was put into which was made up of people with different strengths and great soft skills! 

We initially put measures in place to get a better idea of what was happening. We enabled logging and created an S3 bucket to store the logs. We used Athena to view logs and found a lot of traffic coming from the web (rather than the traffic generator that was simulating genuine requests from the vetinary hospital). Some of these requests appeared to be malicious as they were requesting admin and sitemapping routes. Given the nature of the system (with known legitimate sources of traffic) we used a security group setting to limit incoming traffic to a single ip address. 

Once the source-limiting was done we could see more clearly what errors were present and these were mostly status 500s - server error.

Out project was to improve the reliability of service without having access to the server or UI! All we had was access to the loadbalancer and at first I didn't see how it would be possible to improve the reliability rate. We spent time combing through resources to find clues about how we could improve the reliability and (in conjunction with consulting with coaches) turned our attention to failed http requests. 

We spent more time looking at options for retrying failed requests and went down some blind allies - I learned that http requests must be handled synchronously and therefore SQS (queue systems) are not suitable as they create latency that would cause the http request to fail anyway. 

In the end our team created a new EC2 instance with nginx running on it. We used this as a reverse proxy sitting between the load-balancer and the server to intercept 500 and 502 responses. We had to try several times to get the correct configuration to retry the requests successfully, but once we did the failure rate dropped below 1%, giving us the target reliability of 99%.

